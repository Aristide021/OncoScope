{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d83206",
   "metadata": {},
   "source": [
    "# OncoScope GGUF Model Converter\n",
    "\n",
    "Save your fine-tuned OncoScope Gemma 3n model in GGUF format for Ollama deployment\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/oncoscope/blob/main/oncoscope_save_gguf.ipynb)\n",
    "\n",
    "## üöÄ Quick Start\n",
    "This notebook loads your previously trained OncoScope model and saves it in GGUF format.\n",
    "Run all cells in sequence or use **Runtime > Run all**.\n",
    "\n",
    "### Prerequisites:\n",
    "- You must have already trained your OncoScope model using the training notebook\n",
    "- The trained model checkpoint should be saved in Google Drive\n",
    "- Access to GPU runtime (T4, L4, or A100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f613152a",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment and Dependencies\n",
    "\n",
    "### Mount Google Drive and Verify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47fb0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Mount Google Drive to access your saved model\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify checkpoint exists\n",
    "import os\n",
    "checkpoint_dir = \"/content/drive/MyDrive/oncoscope_checkpoints\"\n",
    "model_path = f\"{checkpoint_dir}/oncoscope-gemma-3n-e4B\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"‚úÖ Found model at: {model_path}\")\n",
    "    print(\"Files in model directory:\")\n",
    "    for file in os.listdir(model_path):\n",
    "        print(f\"  - {file}\")\n",
    "else:\n",
    "    print(f\"‚ùå Model not found at {model_path}\")\n",
    "    print(\"Please ensure you've run the training notebook first!\")\n",
    "    print(\"\\nLooking for alternative checkpoint locations...\")\n",
    "    \n",
    "    # Check for alternative paths\n",
    "    alt_paths = [\n",
    "        f\"{checkpoint_dir}/oncoscope-gemma-3n\",\n",
    "        f\"{checkpoint_dir}/lora_model\",\n",
    "        \"/content/drive/MyDrive/OncoScope/checkpoints\"\n",
    "    ]\n",
    "    \n",
    "    for alt_path in alt_paths:\n",
    "        if os.path.exists(alt_path):\n",
    "            print(f\"Found alternative checkpoint at: {alt_path}\")\n",
    "            model_path = alt_path\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d72ffb",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a0c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# Install Unsloth with Gemma 3n support\n",
    "!pip install --upgrade --force-reinstall \"unsloth[gemma3n] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --upgrade bitsandbytes\n",
    "!pip install --upgrade transformers\n",
    "!pip install --upgrade peft\n",
    "\n",
    "print(\"‚úÖ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5712f82",
   "metadata": {},
   "source": [
    "## Step 2: Load Model and Configuration\n",
    "\n",
    "### GPU Detection and Memory Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "# Auto-detect GPU\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "print(f\"Detected GPU: {gpu_name} ({vram_gb:.1f} GB VRAM)\")\n",
    "\n",
    "# Configure based on GPU\n",
    "if \"A100\" in gpu_name:\n",
    "    max_seq_length = 2048\n",
    "    max_memory = \"40GB\"\n",
    "    print(\"üöÄ A100 detected - Using high performance settings\")\n",
    "elif \"L4\" in gpu_name:\n",
    "    max_seq_length = 1536\n",
    "    max_memory = \"22GB\"\n",
    "    print(\"‚ö° L4 detected - Using optimized settings\")\n",
    "else:  # T4 or smaller\n",
    "    max_seq_length = 1024\n",
    "    max_memory = \"14GB\"\n",
    "    print(\"üíª T4/Standard GPU detected - Using conservative settings\")\n",
    "\n",
    "print(f\"Max sequence length: {max_seq_length}\")\n",
    "print(f\"Max memory allocation: {max_memory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2313e4c3",
   "metadata": {},
   "source": [
    "### Load Base Model and LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d7e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model\n",
    "print(\"\\nüîÑ Loading base Gemma 3n model...\")\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\",\n",
    "    dtype = None,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,\n",
    "    device_map = \"auto\",\n",
    "    max_memory = {0: max_memory, \"cpu\": \"20GB\"},\n",
    ")\n",
    "print(\"‚úÖ Base model loaded!\")\n",
    "\n",
    "# Load the LoRA adapters\n",
    "print(\"\\nüîÑ Loading your fine-tuned LoRA adapters...\")\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers = False,\n",
    "    finetune_language_layers = True,\n",
    "    finetune_attention_modules = True,\n",
    "    finetune_mlp_modules = True,\n",
    ")\n",
    "\n",
    "# Load the saved LoRA weights\n",
    "print(\"üîÑ Loading saved LoRA weights...\")\n",
    "try:\n",
    "    from peft import PeftModel\n",
    "    model = PeftModel.from_pretrained(model, model_path)\n",
    "    print(\"‚úÖ LoRA weights loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not load LoRA weights from {model_path}\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"üí° The model will use base Gemma 3n weights only\")\n",
    "\n",
    "# Try to load custom tokenizer, fallback to base tokenizer if not available\n",
    "print(\"üîÑ Checking for custom tokenizer...\")\n",
    "try:\n",
    "    import os\n",
    "    tokenizer_files = ['tokenizer.json', 'tokenizer_config.json', 'special_tokens_map.json']\n",
    "    has_custom_tokenizer = all(os.path.exists(os.path.join(model_path, f)) for f in tokenizer_files)\n",
    "    \n",
    "    if has_custom_tokenizer:\n",
    "        from transformers import AutoTokenizer\n",
    "        custom_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        tokenizer = custom_tokenizer\n",
    "        print(\"‚úÖ Custom tokenizer loaded from checkpoint!\")\n",
    "    else:\n",
    "        print(\"üí° Using base model tokenizer (no custom tokenizer found)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load custom tokenizer: {e}\")\n",
    "    print(\"üí° Using base model tokenizer\")\n",
    "\n",
    "print(\"‚úÖ Model and adapters loaded successfully!\")\n",
    "print(f\"üìç Model loaded from: {model_path}\")\n",
    "\n",
    "# Show model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"üìä Total parameters: {total_params:,}\")\n",
    "print(f\"üéØ Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"üìà Trainable %: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a82a31",
   "metadata": {},
   "source": [
    "## Step 3: Test Model Before Conversion\n",
    "\n",
    "Let's verify that our fine-tuned model is working correctly before conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32d757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a cancer genomics query\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Ensure we have the right chat template\n",
    "tokenizer = get_chat_template(tokenizer, chat_template = \"gemma-3\")\n",
    "\n",
    "test_query = \"Analyze the clinical significance of BRCA1 c.68_69delAG mutation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": test_query}]}]\n",
    "\n",
    "print(f\"üß™ Test Query: {test_query}\\n\")\n",
    "print(\"üîÑ Generating response...\")\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate response\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    "    top_k=64,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\nü§ñ Model Response:\\n{response}\")\n",
    "\n",
    "# Cleanup to free memory\n",
    "del inputs, outputs\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n‚úÖ Model test successful! Ready for conversion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee239654",
   "metadata": {},
   "source": [
    "## Step 4: Convert to GGUF Format\n",
    "\n",
    "### Multiple Quantization Levels (Recommended)\n",
    "\n",
    "We'll create multiple GGUF versions with different quantization levels:\n",
    "- **Q4_K_M**: Best balance of quality and size (~2.5GB) - Recommended\n",
    "- **Q8_0**: Higher quality, larger file (~4.5GB) - For powerful machines  \n",
    "- **Q5_K_M**: Medium quality (~3.2GB) - Good alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf9730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output directory\n",
    "gguf_output_dir = f\"{checkpoint_dir}/gguf_models\"\n",
    "os.makedirs(gguf_output_dir, exist_ok=True)\n",
    "\n",
    "print(\"üöÄ Saving GGUF models with different quantization levels...\")\n",
    "print(\"‚è∞ This will take 10-30 minutes depending on your GPU.\\n\")\n",
    "\n",
    "# Save Q4_K_M (Recommended - best balance)\n",
    "print(\"1Ô∏è‚É£ Saving Q4_K_M (recommended for most users)...\")\n",
    "try:\n",
    "    # Method 1: Try the basic save_pretrained_gguf method\n",
    "    model.save_pretrained_gguf(\n",
    "        f\"{gguf_output_dir}/oncoscope-gemma-3n-q4_k_m\",\n",
    "        quantization_method=\"q4_k_m\"\n",
    "    )\n",
    "    print(\"‚úÖ Q4_K_M saved!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Method 1 failed: {e}\")\n",
    "    try:\n",
    "        # Method 2: Try with directory only\n",
    "        print(\"üîÑ Trying method 2...\")\n",
    "        model.save_pretrained_gguf(\n",
    "            directory=f\"{gguf_output_dir}/oncoscope-gemma-3n-q4_k_m\",\n",
    "            quantization_method=\"q4_k_m\"\n",
    "        )\n",
    "        print(\"‚úÖ Q4_K_M saved with method 2!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ö†Ô∏è Method 2 failed: {e2}\")\n",
    "        try:\n",
    "            # Method 3: Try positional arguments only\n",
    "            print(\"üîÑ Trying method 3 (positional args)...\")\n",
    "            model.save_pretrained_gguf(\n",
    "                f\"{gguf_output_dir}/oncoscope-gemma-3n-q4_k_m\",\n",
    "                \"q4_k_m\"\n",
    "            )\n",
    "            print(\"‚úÖ Q4_K_M saved with method 3!\")\n",
    "        except Exception as e3:\n",
    "            print(f\"‚ùå All methods failed for Q4_K_M: {e3}\")\n",
    "\n",
    "# Save Q8_0 (Higher quality, larger size)\n",
    "print(\"\\n2Ô∏è‚É£ Saving Q8_0 (higher quality, larger file)...\")\n",
    "try:\n",
    "    # Method 1: Try the basic save_pretrained_gguf method\n",
    "    model.save_pretrained_gguf(\n",
    "        f\"{gguf_output_dir}/oncoscope-gemma-3n-q8_0\",\n",
    "        quantization_method=\"q8_0\"\n",
    "    )\n",
    "    print(\"‚úÖ Q8_0 saved!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Method 1 failed: {e}\")\n",
    "    try:\n",
    "        # Method 2: Try with directory only\n",
    "        print(\"üîÑ Trying method 2...\")\n",
    "        model.save_pretrained_gguf(\n",
    "            directory=f\"{gguf_output_dir}/oncoscope-gemma-3n-q8_0\",\n",
    "            quantization_method=\"q8_0\"\n",
    "        )\n",
    "        print(\"‚úÖ Q8_0 saved with method 2!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ö†Ô∏è Method 2 failed: {e2}\")\n",
    "        try:\n",
    "            # Method 3: Try positional arguments only\n",
    "            print(\"üîÑ Trying method 3 (positional args)...\")\n",
    "            model.save_pretrained_gguf(\n",
    "                f\"{gguf_output_dir}/oncoscope-gemma-3n-q8_0\",\n",
    "                \"q8_0\"\n",
    "            )\n",
    "            print(\"‚úÖ Q8_0 saved with method 3!\")\n",
    "        except Exception as e3:\n",
    "            print(f\"‚ùå All methods failed for Q8_0: {e3}\")\n",
    "\n",
    "# Save Q5_K_M (Medium quality)\n",
    "print(\"\\n3Ô∏è‚É£ Saving Q5_K_M (medium quality)...\")\n",
    "try:\n",
    "    # Method 1: Try the basic save_pretrained_gguf method\n",
    "    model.save_pretrained_gguf(\n",
    "        f\"{gguf_output_dir}/oncoscope-gemma-3n-q5_k_m\",\n",
    "        quantization_method=\"q5_k_m\"\n",
    "    )\n",
    "    print(\"‚úÖ Q5_K_M saved!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Method 1 failed: {e}\")\n",
    "    try:\n",
    "        # Method 2: Try with directory only\n",
    "        print(\"üîÑ Trying method 2...\")\n",
    "        model.save_pretrained_gguf(\n",
    "            directory=f\"{gguf_output_dir}/oncoscope-gemma-3n-q5_k_m\",\n",
    "            quantization_method=\"q5_k_m\"\n",
    "        )\n",
    "        print(\"‚úÖ Q5_K_M saved with method 2!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ö†Ô∏è Method 2 failed: {e2}\")\n",
    "        try:\n",
    "            # Method 3: Try positional arguments only\n",
    "            print(\"üîÑ Trying method 3 (positional args)...\")\n",
    "            model.save_pretrained_gguf(\n",
    "                f\"{gguf_output_dir}/oncoscope-gemma-3n-q5_k_m\",\n",
    "                \"q5_k_m\"\n",
    "            )\n",
    "            print(\"‚úÖ Q5_K_M saved with method 3!\")\n",
    "        except Exception as e3:\n",
    "            print(f\"‚ùå All methods failed for Q5_K_M: {e3}\")\n",
    "\n",
    "print(f\"\\nüéâ GGUF conversion attempts completed!\")\n",
    "print(f\"üìÅ Output directory: {gguf_output_dir}\")\n",
    "\n",
    "# List the files and their sizes\n",
    "import glob\n",
    "gguf_files = glob.glob(f\"{gguf_output_dir}/*/*.gguf\")\n",
    "if gguf_files:\n",
    "    print(\"\\nüìÅ Created GGUF files:\")\n",
    "    for file in gguf_files:\n",
    "        size_mb = os.path.getsize(file) / (1024 * 1024)\n",
    "        filename = os.path.basename(file)\n",
    "        print(f\"  üìÑ {filename} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No GGUF files found. Let's check what was created:\")\n",
    "    # Check all files in output directory\n",
    "    all_files = glob.glob(f\"{gguf_output_dir}/**/*\", recursive=True)\n",
    "    for file in all_files:\n",
    "        if os.path.isfile(file):\n",
    "            size_mb = os.path.getsize(file) / (1024 * 1024)\n",
    "            filename = os.path.relpath(file, gguf_output_dir)\n",
    "            print(f\"  üìÑ {filename} ({size_mb:.1f} MB)\")\n",
    "\n",
    "# Memory cleanup after conversion\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nüßπ Memory cleaned up after conversion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a23462",
   "metadata": {},
   "source": [
    "### Alternative: Quick Single Quantization (Optional)\n",
    "\n",
    "Uncomment and run the cell below if you only want a single GGUF model (Q4_K_M) for faster conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e233d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quick save - just one quantization level\n",
    "# quick_output = f\"{checkpoint_dir}/oncoscope-gemma-3n-gguf\"\n",
    "# \n",
    "# print(\"üöÄ Saving single GGUF model (Q4_K_M)...\")\n",
    "# model.save_pretrained_gguf(\n",
    "#     quick_output,\n",
    "#     tokenizer,\n",
    "#     quantization_method = \"q4_k_m\"\n",
    "# )\n",
    "# print(f\"‚úÖ GGUF model saved to: {quick_output}\")\n",
    "\n",
    "print(\"‚è© Skipping quick conversion - using multiple quantization levels above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f92f93e",
   "metadata": {},
   "source": [
    "## Step 5: Create Ollama Modelfile\n",
    "\n",
    "Generate an optimized Modelfile for Ollama deployment with cancer genomics-specific system prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa6ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Modelfile for Ollama\n",
    "modelfile_content = \"\"\"# OncoScope - Cancer Genomics AI Assistant\n",
    "# Fine-tuned Gemma 3n model for precision oncology\n",
    "\n",
    "FROM ./oncoscope-gemma-3n-q4_k_m.gguf\n",
    "\n",
    "# Model parameters optimized for cancer genomics\n",
    "PARAMETER temperature 1.0\n",
    "PARAMETER top_p 0.95\n",
    "PARAMETER top_k 64\n",
    "PARAMETER repeat_penalty 1.0\n",
    "PARAMETER num_predict 512\n",
    "\n",
    "# System prompt for cancer genomics expertise\n",
    "SYSTEM You are OncoScope, an AI assistant specialized in cancer genomics and precision oncology. You analyze genetic mutations, provide clinical insights, and suggest evidence-based recommendations for cancer treatment. Always prioritize patient safety and clearly indicate when professional medical consultation is needed.\n",
    "\n",
    "# Chat template for Gemma 3n\n",
    "TEMPLATE \\\"\\\"\\\"{{ if .System }}<start_of_turn>system\n",
    "{{ .System }}<end_of_turn>\n",
    "{{ end }}{{ if .Prompt }}<start_of_turn>user\n",
    "{{ .Prompt }}<end_of_turn>\n",
    "{{ end }}<start_of_turn>model\n",
    "{{ .Response }}<end_of_turn>\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "# Stop tokens\n",
    "PARAMETER stop \"<start_of_turn>\"\n",
    "PARAMETER stop \"<end_of_turn>\"\n",
    "\"\"\"\n",
    "\n",
    "# Save Modelfile\n",
    "modelfile_path = f\"{gguf_output_dir}/Modelfile\"\n",
    "with open(modelfile_path, 'w') as f:\n",
    "    f.write(modelfile_content)\n",
    "\n",
    "print(\"üìÑ Ollama Modelfile created!\")\n",
    "print(f\"üìç Saved to: {modelfile_path}\")\n",
    "\n",
    "# Create additional Modelfiles for different quantizations\n",
    "for quant in [\"q8_0\", \"q5_k_m\"]:\n",
    "    alt_modelfile_content = modelfile_content.replace(\"q4_k_m\", quant)\n",
    "    alt_modelfile_path = f\"{gguf_output_dir}/Modelfile-{quant}\"\n",
    "    with open(alt_modelfile_path, 'w') as f:\n",
    "        f.write(alt_modelfile_content)\n",
    "    print(f\"üìÑ Alternative Modelfile created: Modelfile-{quant}\")\n",
    "\n",
    "print(\"\\nüéØ Ollama usage instructions:\")\n",
    "print(\"1. Copy the GGUF file and Modelfile to your local machine\")\n",
    "print(\"2. Run: ollama create oncoscope -f Modelfile\")\n",
    "print(\"3. Run: ollama run oncoscope\")\n",
    "print(\"4. Test: 'Analyze BRCA1 mutation p.Gln356Ter'\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a0f011",
   "metadata": {},
   "source": [
    "## Step 6: Download and Deploy Models\n",
    "\n",
    "### Package Files for Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c27b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all GGUF files and Modelfiles\n",
    "import glob\n",
    "gguf_files = glob.glob(f\"{gguf_output_dir}/*/*.gguf\")\n",
    "modelfiles = glob.glob(f\"{gguf_output_dir}/Modelfile*\")\n",
    "\n",
    "print(\"üì¶ Found files for packaging:\")\n",
    "total_size = 0\n",
    "for file in gguf_files + modelfiles:\n",
    "    size_mb = os.path.getsize(file) / (1024 * 1024)\n",
    "    total_size += size_mb\n",
    "    filename = os.path.basename(file)\n",
    "    print(f\"  üìÑ {filename} ({size_mb:.1f} MB)\")\n",
    "\n",
    "print(f\"\\nüìä Total package size: {total_size:.1f} MB\")\n",
    "\n",
    "# Create a zip file for easy download\n",
    "import zipfile\n",
    "zip_path = f\"{checkpoint_dir}/oncoscope_gguf_models.zip\"\n",
    "\n",
    "print(f\"\\nüì¶ Creating zip archive...\")\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add GGUF files\n",
    "    for file in gguf_files:\n",
    "        arcname = os.path.relpath(file, checkpoint_dir)\n",
    "        zipf.write(file, arcname)\n",
    "        print(f\"  ‚úÖ Added: {arcname}\")\n",
    "    \n",
    "    # Add Modelfiles\n",
    "    for file in modelfiles:\n",
    "        arcname = os.path.relpath(file, checkpoint_dir)\n",
    "        zipf.write(file, arcname)\n",
    "        print(f\"  ‚úÖ Added: {arcname}\")\n",
    "\n",
    "zip_size_mb = os.path.getsize(zip_path) / (1024**2)\n",
    "print(f\"\\nüéâ Zip created: {os.path.basename(zip_path)} ({zip_size_mb:.1f} MB)\")\n",
    "\n",
    "# Download the zip file\n",
    "from google.colab import files\n",
    "print(\"\\nüì• Starting download...\")\n",
    "try:\n",
    "    files.download(zip_path)\n",
    "    print(\"‚úÖ Download initiated!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Download error: {e}\")\n",
    "    print(\"üí° You can manually download from the Files panel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d0c7c",
   "metadata": {},
   "source": [
    "### Deployment Instructions\n",
    "\n",
    "Your OncoScope model is now ready for deployment on various platforms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b4b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment instructions file\n",
    "deployment_instructions = \"\"\"# OncoScope GGUF Deployment Guide\n",
    "\n",
    "## üéâ Conversion Complete!\n",
    "\n",
    "### What You've Created:\n",
    "- **Q4_K_M**: Best balance of quality and size (~2.5GB) - Recommended\n",
    "- **Q8_0**: Higher quality, larger file (~4.5GB) - For powerful machines\n",
    "- **Q5_K_M**: Medium quality (~3.2GB) - Good alternative\n",
    "\n",
    "### Deployment Options:\n",
    "\n",
    "#### 1. **Ollama** (Recommended for local use):\n",
    "```bash\n",
    "# After downloading and extracting files\n",
    "cd path/to/extracted/files\n",
    "ollama create oncoscope -f Modelfile\n",
    "ollama run oncoscope\n",
    "\n",
    "# Test with:\n",
    "# \"Analyze the clinical significance of BRCA1 c.68_69delAG mutation\"\n",
    "```\n",
    "\n",
    "#### 2. **LM Studio**:\n",
    "- Import the GGUF file directly\n",
    "- Use the Gemma 3 chat template\n",
    "- Set temperature=1.0, top_p=0.95\n",
    "\n",
    "#### 3. **llama.cpp**:\n",
    "```bash\n",
    "./main -m oncoscope-gemma-3n-q4_k_m.gguf \\\\\n",
    "  --temp 1.0 --top-p 0.95 --top-k 64 \\\\\n",
    "  -p \"Analyze BRCA1 mutation...\"\n",
    "```\n",
    "\n",
    "#### 4. **Python with ctransformers**:\n",
    "```python\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"path/to/oncoscope-gemma-3n-q4_k_m.gguf\",\n",
    "    model_type=\"gemma\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Performance Tips:\n",
    "- Q4_K_M runs well on 8GB+ RAM\n",
    "- Use GPU acceleration when available\n",
    "- For Apple Silicon: Enable Metal acceleration\n",
    "- For NVIDIA: Enable CUDA\n",
    "\n",
    "### Integration with OncoScope Backend:\n",
    "Update your OncoScope configuration:\n",
    "```python\n",
    "# In backend/config.py\n",
    "OLLAMA_MODEL_NAME = \"oncoscope\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "```\n",
    "\n",
    "### Need Help?\n",
    "- OncoScope GitHub: https://github.com/yourusername/oncoscope\n",
    "- Ollama Documentation: https://ollama.ai/docs\n",
    "- Unsloth Discord: https://discord.gg/unsloth\n",
    "\n",
    "Thank you for using OncoScope! üß¨üè•\n",
    "\"\"\"\n",
    "\n",
    "# Save instructions file\n",
    "instructions_path = f\"{gguf_output_dir}/DEPLOYMENT_INSTRUCTIONS.md\"\n",
    "with open(instructions_path, 'w') as f:\n",
    "    f.write(deployment_instructions)\n",
    "\n",
    "print(\"üìã Deployment instructions created!\")\n",
    "print(f\"üìç Saved to: {instructions_path}\")\n",
    "\n",
    "# Display key information\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ ONCOSCOPE GGUF CONVERSION COMPLETE! üéâ\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÅ Files location: {gguf_output_dir}\")\n",
    "print(f\"üì¶ Download package: {zip_path}\")\n",
    "print(\"\\nüöÄ Ready for deployment!\")\n",
    "print(\"üìñ See DEPLOYMENT_INSTRUCTIONS.md for detailed setup\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee9ddf",
   "metadata": {},
   "source": [
    "## üéâ Conversion Complete!\n",
    "\n",
    "### Summary of What We've Created:\n",
    "\n",
    "‚úÖ **Multiple GGUF Models**: Q4_K_M (recommended), Q8_0 (high quality), Q5_K_M (balanced)  \n",
    "‚úÖ **Ollama Modelfiles**: Optimized parameters for cancer genomics  \n",
    "‚úÖ **Deployment Package**: Compressed zip file with all necessary files  \n",
    "‚úÖ **Instructions**: Comprehensive deployment guide for multiple platforms  \n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Download** the zip file from above\n",
    "2. **Extract** files to your local machine\n",
    "3. **Choose** your deployment platform (Ollama recommended)\n",
    "4. **Test** with sample cancer genomics queries\n",
    "5. **Integrate** with your OncoScope backend\n",
    "\n",
    "### Sample Test Queries:\n",
    "\n",
    "Once deployed, try these queries to test your model:\n",
    "\n",
    "- `\"Analyze the clinical significance of BRCA1 c.68_69delAG mutation\"`\n",
    "- `\"What are the therapeutic implications of KRAS G12C mutation?\"`\n",
    "- `\"Explain the pathogenicity of TP53 R273H in colorectal cancer\"`\n",
    "- `\"What targeted therapies are available for EGFR L858R mutation?\"`\n",
    "\n",
    "### Performance Expectations:\n",
    "\n",
    "- **Response Time**: 2-10 seconds on modern hardware\n",
    "- **Quality**: High accuracy for cancer genomics queries\n",
    "- **Memory Usage**: 3-6GB RAM depending on quantization\n",
    "- **Integration**: Compatible with existing OncoScope infrastructure\n",
    "\n",
    "---\n",
    "\n",
    "**üß¨ Your OncoScope model is now ready for precision oncology! üè•**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
