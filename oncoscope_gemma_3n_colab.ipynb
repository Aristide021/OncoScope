{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OncoScope: Fine-tune Gemma 3n for Cancer Genomics Analysis\n",
    "Modified from Unsloth's official Gemma 3n notebook for cancer mutation analysis\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/oncoscope/blob/main/oncoscope_gemma_3n_colab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Unsloth for Colab\n",
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "!pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "!pip install --no-deps unsloth\n",
    "\n",
    "# Install latest transformers for Gemma 3N\n",
    "!pip install --no-deps --upgrade transformers\n",
    "!pip install --no-deps --upgrade timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Your Cancer Training Data\n",
    "Upload your `cancer_training_data.json` file when prompted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "print(\"Please upload your cancer_training_data.json file...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Load the training data\n",
    "with open('cancer_training_data.json', 'r') as f:\n",
    "    cancer_training_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(cancer_training_data)} cancer genomics training examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Gemma 3n Model (Text-Only Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "# Use E2B (2B) model for T4 GPU compatibility\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n",
    "    dtype = None,  # Auto detection\n",
    "    max_seq_length = 1024,  # Reduced for T4 GPU\n",
    "    load_in_4bit = True,\n",
    "    full_finetuning = False,  # Use LoRA\n",
    ")\n",
    "\n",
    "print(f\"Model loaded! GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure LoRA for Text-Only Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure for text-only cancer genomics\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False,  # Disable vision - saves VRAM!\n",
    "    finetune_language_layers   = True,   # Text only for cancer analysis\n",
    "    finetune_attention_modules = True,   # Good for specialized domain\n",
    "    finetune_mlp_modules       = True,   # Keep for performance\n",
    "    \n",
    "    r = 8,           # LoRA rank - reduced for T4\n",
    "    lora_alpha = 8,  # Match r value\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "print(\"LoRA adapters configured for cancer genomics text analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Cancer Genomics Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template, standardize_data_formats\n",
    "from datasets import Dataset\n",
    "\n",
    "# Setup Gemma 3 chat template\n",
    "tokenizer = get_chat_template(tokenizer, chat_template = \"gemma-3\")\n",
    "\n",
    "# Convert cancer data to conversation format\n",
    "conversations_data = []\n",
    "for example in cancer_training_data:\n",
    "    conversation = [\n",
    "        {\"role\": \"user\", \"content\": example[\"input\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"output\"]}\n",
    "    ]\n",
    "    conversations_data.append({\"conversations\": conversation})\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_list(conversations_data)\n",
    "dataset = standardize_data_formats(dataset)\n",
    "\n",
    "# Apply chat template\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            convo, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=False\n",
    "        ).removeprefix('<bos>')\n",
    "        for convo in convos\n",
    "    ]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Split dataset\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=3407)\n",
    "print(f\"Training samples: {len(dataset['train'])}\")\n",
    "print(f\"Evaluation samples: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"test\"],\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 1,  # T4 limitation\n",
    "        gradient_accumulation_steps = 4,  # Effective batch size = 4\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 3,  # Full training\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 10,\n",
    "        save_steps = 100,\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 50,\n",
    "        optim = \"paged_adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"oncoscope_outputs\",\n",
    "        report_to = \"tensorboard\",  # Can view in Colab\n",
    "        load_best_model_at_end = True,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Train only on assistant responses\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")\n",
    "\n",
    "print(\"Trainer configured for cancer genomics specialization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU memory before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "# Train the model\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Show training stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "print(f\"Training completed in {trainer_stats.metrics['train_runtime']/60:.2f} minutes\")\n",
    "print(f\"Peak memory = {used_memory} GB ({used_percentage}% of max)\")\n",
    "print(f\"Peak memory for training = {used_memory_for_lora} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Cancer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cancer genomics queries\n",
    "test_queries = [\n",
    "    \"Analyze the BRCA1 c.68_69delAG mutation for cancer risk assessment.\",\n",
    "    \"What are the therapeutic implications of KRAS G12C mutation in lung cancer?\",\n",
    "    \"Provide clinical recommendations for a patient with TP53 R175H mutation.\"\n",
    "]\n",
    "\n",
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": query}]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    _ = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=1.0,\n",
    "        top_p=0.95,\n",
    "        top_k=64,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    \n",
    "    # Cleanup\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "model.save_pretrained(\"oncoscope-gemma-3n-lora\")\n",
    "tokenizer.save_pretrained(\"oncoscope-gemma-3n-lora\")\n",
    "\n",
    "# Option 1: Save merged model (16-bit)\n",
    "# model.save_pretrained_merged(\"oncoscope-gemma-3n-merged\", tokenizer)\n",
    "\n",
    "# Option 2: Save as GGUF for Ollama\n",
    "# model.save_pretrained_gguf(\n",
    "#     \"oncoscope-gemma-3n\",\n",
    "#     tokenizer,\n",
    "#     quantization_type = \"Q8_0\"\n",
    "# )\n",
    "\n",
    "print(\"Model saved! You can download the files or push to HuggingFace.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip and download the model\n",
    "!zip -r oncoscope-gemma-3n-lora.zip oncoscope-gemma-3n-lora/\n",
    "files.download('oncoscope-gemma-3n-lora.zip')\n",
    "\n",
    "print(\"Model downloaded! You can now use it locally or deploy it.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}