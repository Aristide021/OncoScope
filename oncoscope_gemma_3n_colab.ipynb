{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OncoScope: Fine-tune Gemma 3n for Cancer Genomics Analysis\n",
    "Modified from Unsloth's official Gemma 3n notebook for cancer mutation analysis\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/oncoscope/blob/main/oncoscope_gemma_3n_colab.ipynb)\n",
    "\n",
    "## üöÄ Quick Start\n",
    "This notebook can be run with **Runtime > Run all** for automatic execution, or step-by-step for more control.\n",
    "- **Auto-configuration**: Detects your GPU and optimizes settings automatically\n",
    "- **Checkpoint persistence**: Saves to Google Drive to prevent work loss\n",
    "- **Production-ready**: Includes both automated and manual configuration options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive (Save Progress!)\n",
    "**IMPORTANT**: Mount Drive first to prevent losing checkpoints if Colab disconnects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create checkpoint directory in Drive\n",
    "import os\n",
    "checkpoint_dir = \"/content/drive/MyDrive/oncoscope_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "print(f\"‚úÖ Checkpoints will be saved to: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies\n",
    "Using Unsloth's official installation method for Gemma 3n support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Unsloth with Gemma 3n support - official recommended method\n",
    "!pip install \"unsloth[gemma3n] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --upgrade \"datasets>=2.16.0\" \"trl>=0.8.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload Your Cancer Training Data\n",
    "Upload your `cancer_training_data.json` file when prompted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "print(\"Please upload your cancer_training_data.json file...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Load the training data (robust file handling)\n",
    "file_name = list(uploaded.keys())[0]\n",
    "with open(file_name, 'r') as f:\n",
    "    cancer_training_data = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(cancer_training_data)} cancer genomics training examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Auto-Detect GPU and Configure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Auto-detect GPU and configure settings\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "\n",
    "print(f\"Detected GPU: {gpu_name}\")\n",
    "print(f\"Available VRAM: {vram_gb:.1f} GB\")\n",
    "\n",
    "# Note: Unsloth handles Conv2D autocast issues for float16 GPUs internally\n",
    "# No manual intervention needed for T4 GPUs\n",
    "\n",
    "# Auto-configure based on GPU\n",
    "if \"A100\" in gpu_name:\n",
    "    batch_size = 4\n",
    "    grad_accum = 1\n",
    "    lora_r = 32\n",
    "    lora_alpha = 64\n",
    "    max_memory = \"40GB\"\n",
    "    max_seq_length = 2048\n",
    "    print(\"‚úÖ A100 detected - Using optimal settings!\")\n",
    "elif \"L4\" in gpu_name:\n",
    "    batch_size = 2\n",
    "    grad_accum = 2\n",
    "    lora_r = 16\n",
    "    lora_alpha = 32\n",
    "    max_memory = \"22GB\"\n",
    "    max_seq_length = 1536\n",
    "    print(\"‚úÖ L4 detected - Using balanced settings\")\n",
    "else:  # T4 or smaller\n",
    "    batch_size = 1\n",
    "    grad_accum = 4\n",
    "    lora_r = 8\n",
    "    lora_alpha = 8\n",
    "    max_memory = \"14GB\"\n",
    "    max_seq_length = 1024\n",
    "    print(\"‚úÖ T4/smaller GPU - Using memory-efficient settings\")\n",
    "    print(\"üìù Note: Unsloth automatically handles float16 Conv2D issues\")\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"- Batch size: {batch_size}\")\n",
    "print(f\"- Gradient accumulation: {grad_accum}\")\n",
    "print(f\"- Effective batch size: {batch_size * grad_accum}\")\n",
    "print(f\"- LoRA rank: {lora_r}\")\n",
    "print(f\"- Max sequence length: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Gemma 3n Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "# Use E2B (2B) model with auto-detected settings\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n",
    "    dtype = None,  # Auto detection\n",
    "    max_seq_length = max_seq_length,  # From auto-config\n",
    "    load_in_4bit = True,\n",
    "    full_finetuning = False,  # Use LoRA\n",
    "    device_map = \"auto\",\n",
    "    max_memory = {0: max_memory, \"cpu\": \"20GB\"},  # From auto-config\n",
    ")\n",
    "\n",
    "print(f\"Model loaded! GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure LoRA Adapters\n",
    "### Option A: Auto-Configured LoRA (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure for text-only cancer genomics with auto-detected settings\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False,  # Disable vision - saves VRAM!\n",
    "    finetune_language_layers   = True,   # Text only for cancer analysis\n",
    "    finetune_attention_modules = True,   # Good for specialized domain\n",
    "    finetune_mlp_modules       = True,   # Keep for performance\n",
    "    \n",
    "    r = lora_r,                # From auto-config\n",
    "    lora_alpha = lora_alpha,   # From auto-config\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "print(f\"LoRA adapters configured with rank={lora_r} for cancer genomics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Manual Override (Advanced Users)\n",
    "Uncomment the code below to manually configure LoRA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUAL OVERRIDE: Use this if you want to override auto-configuration\n",
    "# Uncomment and modify the values below as needed\n",
    "\n",
    "# model = FastModel.get_peft_model(\n",
    "#     model,\n",
    "#     finetune_vision_layers     = False,\n",
    "#     finetune_language_layers   = True,\n",
    "#     finetune_attention_modules = True,\n",
    "#     finetune_mlp_modules       = True,\n",
    "#     \n",
    "#     r = 8,           # LoRA rank - manual override\n",
    "#     lora_alpha = 8,  # Match r value\n",
    "#     lora_dropout = 0,\n",
    "#     bias = \"none\",\n",
    "#     random_state = 3407,\n",
    "# )\n",
    "# \n",
    "# print(\"LoRA adapters manually configured for cancer genomics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prepare Cancer Genomics Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template, standardize_data_formats\n",
    "from datasets import Dataset\n",
    "\n",
    "# Setup Gemma 3 chat template\n",
    "tokenizer = get_chat_template(tokenizer, chat_template = \"gemma-3\")\n",
    "\n",
    "# Convert cancer data to conversation format (CORRECT FORMAT)\n",
"conversations_data = [\n",
"    {\n",
"        'conversations': [\n",
"            {'role': 'user', 'content': ex['input']},\n",
"            {'role': 'assistant', 'content': ex['output']},\n",
"        ]\n",
"    } for ex in cancer_training_data\n",
"]\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_list(conversations_data)\n",
    "dataset = standardize_data_formats(dataset)\n",
    "\n",
    "# Apply chat template\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            convo, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=False\n",
    "        ).removeprefix('<bos>')\n",
    "        for convo in convos\n",
    "    ]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Split dataset\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=3407)\n",
    "print(f\"Training samples: {len(dataset['train'])}\")\n",
    "print(f\"Evaluation samples: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Setup Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "# Check for existing checkpoints\n",
    "import glob\n",
    "existing_checkpoints = glob.glob(f\"{checkpoint_dir}/checkpoint-*\")\n",
    "resume_from_checkpoint = True if existing_checkpoints else False\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "    print(f\"‚úÖ Found {len(existing_checkpoints)} checkpoints - will resume from latest!\")\n",
    "else:\n",
    "    print(\"üìù No checkpoints found - starting fresh training\")\n",
    "\n",
    "# Use modern TrainingArguments with automatic bf16/fp16 detection\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = checkpoint_dir,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    gradient_accumulation_steps = grad_accum,\n",
    "    warmup_steps = 5,\n",
    "    num_train_epochs = 3,\n",
    "    learning_rate = 2e-4,\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    logging_steps = 10,\n",
    "    save_steps = 100,\n",
    "    eval_strategy = \"steps\",\n",
    "    eval_steps = 50,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 3407,\n",
    "    report_to = \"tensorboard\",\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = \"eval_loss\",\n",
    "    greater_is_better = False,\n",
    "    save_total_limit = 3,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"test\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = training_args,\n",
    ")\n",
    "\n",
    "# Train only on assistant responses\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")\n",
    "\n",
    "print(f\"Trainer configured with effective batch size = {batch_size * grad_accum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Start Training\n",
    "### Option A: Full Training with Monitoring (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU memory before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "# Estimate training time\n",
    "if \"A100\" in gpu_stats.name:\n",
    "    estimated_time = \"1-2 hours\"\n",
    "elif \"L4\" in gpu_stats.name:\n",
    "    estimated_time = \"3-4 hours\"\n",
    "else:\n",
    "    estimated_time = \"10-12 hours\"\n",
    "print(f\"\\n‚è∞ Estimated training time: {estimated_time}\")\n",
    "print(\"üíæ Checkpoints auto-save to Google Drive every 100 steps\")\n",
    "print(\"üîÑ Training will auto-resume if disconnected\\n\")\n",
    "\n",
    "# IMPORTANT: Gemma 3n starts with high losses (6-7) - this is normal!\n",
    "print(\"‚ö†Ô∏è NOTE: Initial loss will be high (6-7) for Gemma 3n - this is expected!\")\n",
    "print(\"The loss will drop rapidly in the first few steps.\\n\")\n",
    "\n",
    "# Train the model\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "# Show training stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "print(f\"\\nTraining completed in {trainer_stats.metrics['train_runtime']/60:.2f} minutes\")\n",
    "print(f\"Peak memory = {used_memory} GB ({used_percentage}% of max)\")\n",
    "print(f\"Peak memory for training = {used_memory_for_lora} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Quick Training Start (Minimal Output)\n",
    "Uncomment the code below for faster execution without detailed monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick start training (minimal output)\n",
    "# Uncomment below for quick training without detailed monitoring\n",
    "\n",
    "# print(\"Starting training...\")\n",
    "# trainer_stats = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "# print(f\"‚úÖ Training completed in {trainer_stats.metrics['train_runtime']/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Model & Preview Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cancer genomics queries with Gemma 3n official settings\n",
    "test_queries = [\n",
    "    \"Analyze the BRCA1 c.68_69delAG mutation for cancer risk assessment.\",\n",
    "    \"What are the therapeutic implications of KRAS G12C mutation in lung cancer?\",\n",
    "    \"Provide clinical recommendations for a patient with TP53 R175H mutation.\"\n",
    "]\n",
    "\n",
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": query}]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Use Gemma 3n official inference settings with CRITICAL do_sample=True\n",
    "    _ = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=1.0,          # Gemma team recommended\n",
    "        top_p=0.95,              # Gemma team recommended\n",
    "        top_k=64,                # Gemma team recommended\n",
    "        min_p=0.0,               # Official setting\n",
    "        repetition_penalty=1.0,   # Official setting\n",
    "        do_sample=True,          # CRITICAL: Must be True for sampling to work!\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    \n",
    "    # Cleanup\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save the Model\n",
    "### Option A: Save to Google Drive (Persistent - Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters to Drive\n",
    "save_path = f\"{checkpoint_dir}/oncoscope-gemma-3n-final\"\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved to Google Drive: {save_path}\")\n",
    "\n",
    "# Option 1: Save merged model (16-bit) - recommended for A100\n",
    "if \"A100\" in torch.cuda.get_device_name(0):\n",
    "    print(\"Saving merged model (A100 has enough memory)...\")\n",
    "    model.save_pretrained_merged(f\"{save_path}-merged\", tokenizer, save_method=\"merged_16bit\")\n",
    "\n",
    "# Option 2: Save as GGUF for Ollama\n",
    "# print(\"Saving GGUF format for local deployment...\")\n",
    "# model.save_pretrained_gguf(\n",
    "#     f\"{save_path}-gguf\",\n",
    "#     tokenizer,\n",
    "#     quantization_type = \"Q8_0\"\n",
    "# )\n",
    "\n",
    "print(\"\\nüì¶ Files saved to Google Drive - safe from disconnections!\")\n",
    "print(\"üí° You can access them at /content/drive/MyDrive/oncoscope_checkpoints/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Save Locally & Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters locally for immediate download\n",
    "local_save_path = \"oncoscope-gemma-3n-lora\"\n",
    "model.save_pretrained(local_save_path)\n",
    "tokenizer.save_pretrained(local_save_path)\n",
    "\n",
    "# Zip the model for easy download\n",
    "!zip -r {local_save_path}.zip {local_save_path}/\n",
    "\n",
    "# Download the zip file\n",
    "from google.colab import files\n",
    "files.download(f'{local_save_path}.zip')\n",
    "\n",
    "print(f\"‚úÖ Model downloaded as {local_save_path}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "1. **Deploy with Ollama**: Convert to GGUF format (uncomment code in Section 11)\n",
    "2. **Share on HuggingFace**: Push your model to the Hub\n",
    "3. **Production Deployment**: Use the saved model in your cancer genomics application\n",
    "\n",
    "### Key Features of The Model:\n",
    "- Specialized for cancer mutation analysis (BRCA1/2, TP53, KRAS, etc.)\n",
    "- Trained on 6,000+ expert-curated examples\n",
    "- Optimized for clinical decision support\n",
    "- Ready for real-world deployment\n",
    "\n",
    "### Resources:\n",
    "- [OncoScope Documentation](https://github.com/yourusername/oncoscope)\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Gemma 3n Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune)\n",
    "\n",
    "Thank you for using OncoScope! Together, we're making precision oncology accessible to everyone. üß¨üè•"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
